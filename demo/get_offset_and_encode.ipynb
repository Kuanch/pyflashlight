{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from pyhandle.dataset.dataloader import TorchLoader\n",
    "from pyhandle.net.intermediate import IntermediateNetwork\n",
    "\n",
    "from net.ssd import SSD300, MultiBoxLoss\n",
    "from utils.obj_utils import cxcy_to_xy, cxcy_to_gcxgcy, xy_to_cxcy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.37s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_root = '/home/sixigma/workplace/meow/coco_data/'\n",
    "coco = torchvision.datasets.coco.CocoDetection(root=coco_root + 'train/train2017/', annFile=coco_root + 'annotations/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = IntermediateNetwork('resnet50', [5, 6])\n",
    "ssd_net = SSD300(resnet, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sixigma/.local/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "priors = cxcy_to_xy(ssd_net.priors_cxcy)\n",
    "multibox = MultiBoxLoss(priors)\n",
    "boxes = []\n",
    "labels = []\n",
    "width, height = coco[1][0].size\n",
    "for obj in range(len(coco[1][1])):\n",
    "    # coco bounding box format [top left x position, top left y position, width, height]\n",
    "    box = coco[1][1][obj]['bbox']\n",
    "    box = [box[0] / width, box[1] / height, box[0] / width + box[2] / width, box[1] / height + box[3] / height]\n",
    "    boxes.append(box)\n",
    "    labels.append(coco[1][1][obj]['category_id'])\n",
    "t_boxes = torch.FloatTensor([boxes]).to(device)\n",
    "t_labels = torch.FloatTensor([labels]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6024, 0.1409, 0.9383, 0.8385],\n",
       "          [0.0828, 0.8368, 0.2891, 0.9664]]], device='cuda:0'),\n",
       " tensor([[25., 25.]], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_boxes, t_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(transforms.Resize((300, 300))((coco[0][0])))\n",
    "t_image = torch.from_numpy(image).permute(2, 0, 1).float().cuda()\n",
    "locs, cls = ssd_net(transforms.Normalize(0, 255)(t_image).expand([1, -1, -1, -1]))\n",
    "locs = locs.to(device)\n",
    "cls = cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30.1980, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = multibox(locs, cls, t_boxes, t_labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intersection bounding box left_top and right_down coordinate\n",
    "lower_bounds = torch.max(t_boxes[0, :, :2].unsqueeze(1), priors[:, :2].unsqueeze(0))\n",
    "upper_bounds = torch.min(t_boxes[0, :, 2:].unsqueeze(1), priors[:, 2:].unsqueeze(0))\n",
    "\n",
    "# Get intersection bounding box width and height\n",
    "intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)\n",
    "\n",
    "# Get volumn of intersection bounding box\n",
    "intersection_area = intersection_dims[:, :, 0] * intersection_dims[:, :, 1]\n",
    "\n",
    "# \n",
    "areas_set_1 = (t_boxes[0, :, 2] - t_boxes[0, :, 0]) * (t_boxes[0, :, 3] - t_boxes[0, :, 1])\n",
    "areas_set_2 = (priors[:, 2] - priors[:, 0]) * (priors[:, 3] - priors[:, 1])\n",
    "\n",
    "union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection_area\n",
    "iou = intersection_area / union  # shape (n_obj, 8732)\n",
    "\n",
    "overlap_for_each_prior, object_for_each_prior = iou.max(dim=0)\n",
    "_, prior_for_each_object = iou.max(dim=1) # (N_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fallowing 2 lines would ensure all objects in this image would map to a prior\n",
    "# Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
    "object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(len(boxes))).to(device)\n",
    "# To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
    "overlap_for_each_prior[prior_for_each_object] = 1.\n",
    "\n",
    "# Labels for each prior\n",
    "label_for_each_prior = t_labels[0][object_for_each_prior]  # (8732)\n",
    "# Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
    "label_for_each_prior[overlap_for_each_prior < 0.5] = 0  # (8732)\n",
    "\n",
    "# Store\n",
    "true_classes = label_for_each_prior\n",
    "\n",
    "# Encode center-size object coordinates into the form we regressed predicted boxes to\n",
    "offset_locs = cxcy_to_gcxgcy(xy_to_cxcy(t_boxes[0][object_for_each_prior]), ssd_net.priors_cxcy)  # (8732, 4)\n",
    "\n",
    "# Identify priors that are positive (object/non-background)\n",
    "positive_priors = true_classes != 0  # (N, 8732)\n",
    "\n",
    "# LOCALIZATION LOSS\n",
    "\n",
    "# Localization loss is computed only over positive (non-background) priors\n",
    "loc_loss = torch.nn.SmoothL1Loss()(locs[0][positive_priors], offset_locs[positive_priors])  # (), scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
